commit df5d5dddf4b1cb6bcb7b2ac99ae4955599155483
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Wed Dec 4 04:39:36 2024 +0200

    fix(cash_forecast_model): Define `s3_prefix` correctly and update SageMaker API usage
    
    - **Define `s3_prefix` before its first use** to prevent `NameError` when uploading files to S3.
    - **Replace `create_auto_ml_job_v2` with `create_auto_ml_job`** in the SageMaker client to ensure compatibility with the current Boto3 SDK.
    - **Ensure scaling metadata is uploaded only once** after `s3_prefix` is properly defined, eliminating redundant uploads.
    - **Enhance logging** by adding detailed exception information and using appropriate logging levels (`INFO`, `ERROR`, `DEBUG`).
    - **Improve configuration handling** by validating all required parameters and ensuring robust error handling throughout the pipeline.
    - **Refactor AutoML job configuration** to align with SageMaker's expected parameters, ensuring the AutoML job initiates and monitors correctly.
    - **Clean up and organize code structure** for better readability and maintainability.
    
    These changes address critical programmatic issues, enhance the robustness of the cash forecasting pipeline, and ensure seamless integration with AWS services.

commit 83452fd2c5575a496f3d5c4047d3b0f0fcb242d4
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Tue Dec 3 08:47:27 2024 +0200

    refactor: Enhance scaling metadata handling and improve pipeline robustness
    
    - **Add `save_scaling_metadata` and `load_scaling_metadata` functions** in `cash_forecast_lib.py` to manage `scaling_metadata.json` consistently.
    - **Update `cash_forecast_model.py`:**
      - Utilize `save_scaling_metadata` for serializing and saving scaling metadata.
      - Remove redundant manual saving of `scaling_metadata.json`.
      - Ensure all scaling-related operations leverage dedicated library functions.
    - **Refactor `cash_forecast_inference.py`:**
      - Integrate `load_scaling_metadata` to load and validate `scaling_metadata.json` from S3.
      - Eliminate duplicate `_load_scaling_parameters` method.
      - Enhance error handling and logging for robust inference processing.
    - **General Improvements:**
      - Remove duplicate and redundant code across scripts to maintain clarity and prevent conflicts.
      - Strengthen error handling with comprehensive try-except blocks and detailed logging.
      - Ensure consistent variable naming and definitions for better readability.
      - Adhere to best practices by maintaining clear documentation, type annotations, and modular function structures.
    
    These refactorings streamline the handling of scaling parameters and metadata, improve maintainability, and bolster the overall reliability of the cash forecasting pipeline.

commit eceb5b090a8154952c6d31a7c432167480c88f3b
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Tue Dec 3 07:38:10 2024 +0200

    Complete model-building workflow with AutoML job monitoring and best model retrieval
    
    - **Enhanced `cash_forecast_lib.py`:**
      - **Added `monitor_auto_ml_job`:** Function to continuously monitor the status of SageMaker AutoML jobs until completion, failure, or stoppage.
      - **Added `retrieve_best_model`:** Function to fetch the best-performing model candidate from a completed AutoML job and create a SageMaker model.
      - **Improved Error Handling:** Enhanced exception handling within new functions to ensure robust pipeline execution.
    
    - **Updated `cash_forecast_model.py`:**
      - **Integrated Job Monitoring:**
        - After initiating the AutoML job, the script now calls `monitor_auto_ml_job` to track the job's progress.
        - Logs the status updates and handles job completion statuses appropriately.
      - **Implemented Best Model Retrieval:**
        - Upon successful completion of the AutoML job, the script invokes `retrieve_best_model` to obtain the best model candidate.
        - Creates a SageMaker model with a unique name based on the country code and timestamp to prevent naming conflicts.
      - **Enhanced Logging:**
        - Added detailed logging for each new step to facilitate easier debugging and traceability.
      - **Error Handling Enhancements:**
        - Introduced exception raising for critical failures during job monitoring and model retrieval to halt the pipeline gracefully.
    
    - **Configuration and Flexibility:**
      - Ensured that all relevant parameters (e.g., `max_wait_time`, `sleep_time`) for job monitoring are configurable via `config.yaml` for greater flexibility.
    
    - **Code Maintainability:**
      - Leveraged the shared library (`cash_forecast_lib.py`) for new functionalities, promoting code reuse and reducing redundancy.
      - Maintained consistent handling of categorical variables (`BranchId`, `ProductId`) across the pipeline by enforcing string data types.
    
    - **Documentation and Readability:**
      - Updated docstrings and comments within both the shared library and model-building script to reflect the new functionalities and their purposes.
    
    - **Miscellaneous:**
      - Ensured that the script checks for existing models with the same name before creation to avoid accidental overwrites.
      - Added placeholder comments for optional steps like model deployment, allowing for future expansions.
    
    This commit finalizes the model-building pipeline by

commit 854b7009b06bc4a3da9a5e23ff68fc4ac18ea1b4
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Tue Dec 3 07:28:08 2024 +0200

    Refactor forecasting scripts with shared library and enable local inference outputs
    
    - **Added `cash_forecast_lib.py`:**
      - Centralized common functionalities including logging setup, S3 interactions, data preparation, scaling operations, and SageMaker Batch Transform job management.
      - Introduced `ScalingParameters` dataclass for structured scaling parameter handling.
      - Implemented utility functions for submitting and monitoring Batch Transform jobs, downloading forecast results, and saving forecasts locally.
    
    - **Updated `cash_forecast_model.py`:**
      - Integrated shared library functions for data loading, preprocessing, scaling, and S3 uploads.
      - Removed redundant code by leveraging `cash_forecast_lib.py` utilities.
      - Ensured categorical variables (`BranchId`, `ProductId`) are treated as strings to maintain their categorical nature.
    
    - **Enhanced `cash_forecast_inference.py`:**
      - Utilized shared library for loading scaling parameters and metadata from S3.
      - Implemented Batch Transform job submission and monitoring using shared utilities.
      - Added functionality to download forecast results from S3, apply inverse scaling, and save final forecasts locally in `final_forecast.csv`.
      - Ensured consistent handling of categorical variables across the inference pipeline.
    
    - **Configuration and Logging:**
      - Maintained comprehensive logging throughout both scripts for better traceability and debugging.
      - Ensured all configurable parameters are managed via `config.yaml` for flexibility.
    
    - **Miscellaneous:**
      - Updated `config.yaml` with necessary configurations such as instance types, forecast horizons, and quantiles.
      - Ensured all scripts are scalable and maintainable, facilitating easy addition of new countries or adjustments to forecasting parameters.
    
    This refactoring enhances code maintainability, reduces redundancy, and ensures a consistent and efficient workflow for both model building and inference processes in the Cash Forecasting system.

commit 033b32bcd26019263cc9c1be23786f03360b7b15
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Tue Dec 3 05:19:40 2024 +0200

    fix(inference): Fix data type error in forecast scaling
    
    - Add explicit column assignment and type conversion after CSV read
    - Improve error handling and logging in _get_forecast_result
    - Fix inverse scaling logic with proper type validation
    - Add debug logging for data types
    - Clean up metadata column assignment
    
    Root cause: Model output CSV was being read without headers, leading to
    data type mismatch during scaling operations. Quantile columns were not
    properly typed before arithmetic operations.
    
    Bug: TypeError: can't multiply sequence by non-int of type 'float'

commit f1327339ef0c759ad2294508de6ed0e406771a96
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Tue Dec 3 05:18:22 2024 +0200

    Fix inference pipeline to handle CSV model outputs and improve error handling
    
    - Updated the `_run_batch_transform_job` method:
      - Set the `Accept` parameter in `TransformOutput` to `'text/csv'` to match the model's output format.
      - Ensured that the `ContentType` parameter in `TransformInput` is correctly set to `'text/csv'`.
    
    - Modified the `_get_forecast_result` method:
      - Changed the logic to read the output files as CSV instead of JSON Lines.
      - Removed the JSON parsing and used `pd.read_csv` to read the model's output.
      - Assigned column names to the DataFrame based on `self.config.quantiles`.
      - Ensured that the quantile columns are converted to numeric types using `pd.to_numeric`.
    
    - Improved error handling and logging:
      - Added checks to handle empty output files and provide informative error messages.
      - Logged detailed information when mismatches occur between inference data and forecast results.
    
    - Provided instructions on accessing SageMaker Batch Transform job logs via AWS CLI:
      - Included steps to list transform jobs, describe a specific job, and retrieve log events.
      - Added commands to download and inspect output files from S3 for debugging purposes.
    
    - Ensured that the inference pipeline correctly processes forecast results and generates the final forecasts.
    
    - Updated documentation and comments to reflect changes in the codebase.

commit ec22d0bf40d84d2a26cc0f4416cfc37d8585717b
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Mon Dec 2 06:18:22 2024 +0200

    fix(forecast): Improve DataFrame merging strategy in forecast results
    
    - Replace pd.concat with column-by-column assignment for merging
    - Add identification of forecast-specific columns
    - Preserve all inference data while adding forecast results
    - Add detailed error logging with stack traces
    - Maintain data type consistency for key columns
    
    This change reduces the risk of column duplication and ensures
    all inference metadata is preserved in the final forecast results.

commit d57677084f010a18c2af4034bfa318285ddae40b
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Mon Dec 2 05:23:28 2024 +0200

    fix(forecast): Enhance DataFrame merging and type handling in forecast results
    
    - Add EffectiveDate to preserved columns during DataFrame concatenation
    - Enforce consistent data types for ProductId, BranchId, and Currency
    - Add column verification logging after DataFrame merge
    - Maintain existing inverse scaling and cleanup functionality
    - Keep consistent error handling throughout processing
    
    This ensures proper data type consistency and column preservation
    when processing forecast results.

commit 5da149f303ec41e2989430f3b29da6d772433ee5
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Mon Dec 2 05:14:53 2024 +0200

    feat(forecast): Enhance inference pipeline with robust data handling
    
    - Add validation and debugging for forecast pipeline inputs
    - Improve scaling parameters loading with better error handling
    - Enhance batch transform job monitoring and results processing
    - Add DataFrame concatenation to preserve inference metadata
    - Fix forecast date assignment in results processing
    - Update common utilities for shared functionality
    - Remove optional forecasting step from training script
    - Add logging improvements across the pipeline
    
    The changes focus on making the inference pipeline more robust while
    maintaining data integrity through the forecasting process. Key improvements
    include better error handling, data validation, and proper combination of
    inference metadata with forecast results.

commit 716151b400a8d98541f82b2f5399386e43e202d5
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Thu Nov 28 18:18:51 2024 +0200

    Refactor scripts to eliminate redundancy and standardize configurations
    
    - Created `common.py` to house shared configurations, logging setup, and utility functions:
      - Moved `ConfigModel` and `Config` classes to `common.py` for consistent configuration management.
      - Extracted `safe_s3_upload` and `load_scaling_parameters` utility functions to promote reuse.
      - Implemented a unified `setup_logging` function to standardize logging across scripts.
    
    - Updated `cash_forecast_model.py`:
      - Imported shared components from `common.py`.
      - Removed duplicated configuration and logging setup code.
      - Utilized shared utility functions for S3 uploads and scaling parameter loading.
    
    - Updated `cash_forecast_inference.py`:
      - Imported shared components from `common.py`.
      - Removed redundant configuration and logging setup code.
      - Leveraged shared utility functions to handle S3 interactions and scaling parameters.
    
    - Enhanced error handling and validation:
      - Centralized configuration loading and validation in `common.py` to ensure consistency.
      - Improved logging messages for better traceability and debugging.
    
    - Improved code maintainability and adherence to Pythonic standards:
      - Organized code into classes with clear responsibilities.
      - Reduced code duplication, making future updates and scalability more manageable.
      - Ensured consistent formatting and structure across both training and inference scripts.

commit 81b583f2a38e1373f39c28ae381188ba32d5ab68
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Thu Nov 28 08:37:57 2024 +0200

    fix: Resolve inference errors and enhance data validation and logging
    
    - **Exclude 'Demand' Column During Inference:**
      Updated the `forecast` method to drop the 'Demand' column when preparing inference data, preventing the model from encountering `NaN` values that lead to "ERROR" outputs.
    
    - **Ensure Data Type Consistency:**
      Verified that all date columns (`EffectiveDate`, `ForecastDate`) are correctly formatted without time zones and that categorical columns (`ProductId`, `BranchId`, `Currency`) are of type `str`, aligning inference data with training data expectations.
    
    - **Validate Scaling Parameters:**
      Confirmed that scaling parameters are accurately loaded from S3 and properly applied during both training and inference phases, maintaining data consistency and integrity.
    
    - **Enhance Logging Mechanism:**
      Improved logging to include detailed information about transform job statuses, file processing steps, and content previews of faulty files. This facilitates easier debugging and monitoring of the pipeline's operations.
    
    - **Improve Error Handling:**
      Enhanced error handling in the `_get_forecast_result` method to capture and log specific issues, including cases where no numeric columns are found in forecast outputs. Added previews of problematic file contents to aid in troubleshooting.
    
    - **Adjust Pivot Table Aggregation:**
      Specified explicit aggregation functions in pivot tables to ensure correct alignment and consolidation of forecast data, preventing mismatches and data inconsistencies.
    
    - **Handle Transform Job Outputs:**
      Updated the `_get_forecast_result` method to accurately identify and process numeric forecast columns. Implemented checks to handle scenarios where expected numeric columns are missing, logging appropriate warnings and errors.
    
    - **Implement Cleanup Procedures:**
      Added cleanup steps to remove temporary directories after processing forecast outputs, ensuring a clean working environment and preventing accumulation of temporary files.
    
    - **Refactor Inference Data Preparation:**
      Modified the `forecast` method to exclude the 'Demand' column and ensure that inference data strictly contains necessary features for prediction, aligning with the model's requirements.
    
    These changes address the issue of the model returning "ERROR" strings during inference by ensuring correct data preparation, scaling, and robust logging for effective troubleshooting. The enhancements also improve the pipeline's reliability, maintainability, and scalability.

commit 520d8d12dc400aba3026e3edb536bda59035b539
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Wed Nov 27 09:41:01 2024 +0200

    feat: Enhance inference pipeline with robust scaling, data validation, and comprehensive logging
    
    - **Load Scaling Parameters from S3:** Implemented `_load_scaling_parameters` method to retrieve and validate scaling parameters directly from S3, ensuring consistency between training and inference phases.
    
    - **Enhanced Data Preparation:**
      - Added validation for required columns in the inference template.
      - Ensured data type consistency for key columns (`ProductId`, `BranchId`, `Currency`).
      - Validated currency-branch combinations against loaded scaling parameters to prevent mismatches.
      - Incorporated effective date parsing and validation for accurate forecast date generation.
    
    - **Comprehensive Forecast Result Processing:**
      - Implemented `_get_forecast_result` to download, combine, and inverse scale forecast outputs from SageMaker batch transform jobs.
      - Ensured accurate inverse scaling using the original scaling parameters.
      - Added detailed statistical report generation (`_generate_statistical_report`) to provide insights into forecast quality and data integrity.
    
    - **Robust Forecast Saving with Validation:**
      - Enhanced `_save_forecasts` to pivot forecast data, rename columns appropriately, and validate the integrity of saved files.
      - Implemented metadata generation to document forecast details and validation outcomes.
    
    - **Improved Logging and Error Handling:**
      - Set up comprehensive logging across all methods for better monitoring and debugging.
      - Enhanced error handling to include rollback mechanisms in case of failures during the inference process.
    
    - **Refactored Main Function:**
      - Strengthened argument validation to ensure correct input formats and the existence of necessary files.
      - Streamlined processing of multiple countries with isolated error handling to prevent pipeline halts.
    
    - **Code Refactoring and Best Practices:**
      - Organized code into clear, maintainable methods with descriptive docstrings.
      - Ensured consistent naming conventions and structured logging for ease of maintenance.
    
    - **Additional Enhancements:**
      - Added cleanup procedures to remove temporary directories post-processing.
      - Ensured scalability and flexibility for handling varying numbers of countries and forecast horizons.
    
    This commit significantly improves the reliability, maintainability, and scalability of the cash forecasting inference pipeline, aligning it closely with best practices and ensuring accurate and actionable forecast outputs.

commit 7f09e37488fc39e3e8fdb323c0845cba993cde6d
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Wed Nov 27 09:35:09 2024 +0200

    feat(inference): implement robust cash forecast inference pipeline
    
    This commit adds a comprehensive inference pipeline for cash forecasting with
    improved scaling, validation, and error handling.
    
    Major Changes:
    - Add ScalingState class for tracking and rollback capability
    - Implement comprehensive data validation and scaling parameter checks
    - Add detailed statistical reporting and forecast validation
    - Enhance logging with structured output and error tracking
    - Add robust file management with safety checks
    
    Technical Details:
    - Add currency-branch combination validation against scaling parameters
    - Implement working copy pattern to prevent data modification
    - Add numerical conversion issue tracking and reporting
    - Add proper date handling and validation
    - Add file existence and size validation
    - Add metadata tracking for forecasts
    - Add outlier detection and validation flags
    - Add process flow control with proper error handling
    
    Testing:
    - Add argument validation for CLI parameters
    - Add configuration validation
    - Add file path validation
    - Add statistical validation of outputs
    
    Breaking Changes:
    - New required arguments: model_timestamp and effective_date
    - Changed output directory structure for better organization
    - Modified scaling parameter format requirements
    
    This commit addresses scaling consistency issues and adds proper validation
    throughout the inference pipeline.

commit e09dc948a289011eb4b2a6fb98b5ae4340d569c5
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Mon Nov 25 09:05:26 2024 +0200

    fix(model): Fix control flow and add scaling restoration in forecast processing
    
    - Remove incorrect nested try block in _get_forecast_result
    - Add scaling restoration after forecast processing
    - Add validation of restored values with detailed logging
    - Preserve scaled values for comparison
    - Add scaling metadata to output DataFrame
    
    The function now properly handles the end-to-end process of:
    1. Loading scaling parameters
    2. Processing forecast files
    3. Restoring original scale
    4. Validating restored values
    5. Cleanup of temporary files
    
    BREAKING CHANGE: Requires scaling parameters from training phase.
    Previous forecasts without scaling parameters will fail.
    
    Related: #234
    Closes: #567

commit 84f8119924ed3c8233f8a6842b670bd6e3e87aa3
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Mon Nov 25 08:48:50 2024 +0200

    fix(model): Implement group-level standardization scaling for cash forecasting
    
    - Add per Currency-BranchId group standardization scaling to Demand values
    - Store scaling parameters and metadata for inference reproducibility
    - Handle edge cases like zero standard deviation groups
    - Add comprehensive logging of scaling operations
    
    BREAKING CHANGE: Training data is now scaled per Currency-BranchId group.
    Existing models will need retraining with updated scaling.
    
    Related: #234
    Closes: #567

commit 18b35990560e6708d8269c933d17d4332c2954fc
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Fri Nov 22 07:54:34 2024 +0200

    feat(pipeline): Add state management and checkpointing
    
    Implement pipeline state persistence to allow resuming interrupted jobs:
    - Add JSON-based state tracking for each pipeline stage
    - Implement checkpointing for data preparation, training, and forecasting
    - Add state file management with load/save functionality
    - Add resume capability for interrupted processes
    
    Technical changes:
    - Add state_file parameter to CashForecastingPipeline
    - Add _load_state() and _save_state() methods
    - Modify run_pipeline() to check state before each step
    - Add state tracking for:
      * Data preparation
      * S3 uploads
      * Model training
      * Forecasting
    
    Added CLI flag:
    --resume: Resume pipeline from last checkpoint
    
    Performance: O(1) state operations, no impact on core pipeline

commit c777f5795deab664b858a4230523ab1385cb63a8
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Fri Nov 22 07:44:53 2024 +0200

    feat(data): Add Pythonic DataFrame filtering methods
    
    Add multiple approaches to filter demand data by branch, product and currency:
    - Boolean masking for explicit filtering
    - Query method for readable syntax
    - Dictionary-based approach for dynamic conditions
    
    Example filters:
    BranchId: 11
    ProductId: 18
    Currency: USD
    
    Performance: All methods O(n) complexity

commit 14a90f3e9593e6f8e2a3ea65b1f22c01d9c0d6ee
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Thu Nov 21 09:12:21 2024 +0200

    Fix aggregation error by converting quantile columns to numeric
    
    - In the `_save_forecasts` method, added conversion of quantile columns (`p10`, `p50`, `p90`) to numeric types using `pd.to_numeric` with `errors='coerce'`.
    - This ensures that the pivot_table aggregation function operates on numeric data, resolving the "agg function failed [how->mean,dtype->object]" error.
    - Enhanced error handling to log and raise a clear exception if quantile conversion fails.

commit 1acb42bb402a1313821b1b0dd45d150e0e629641
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Thu Nov 21 08:31:03 2024 +0200

    fix: resolve column mismatch in forecast output processing
    
    - Add robust column detection for forecast output files
    - Implement numeric column validation and mapping to quantiles
    - Enhance error logging with file content preview and shape info
    - Add data alignment checks before concatenation
    - Fix row count mismatch handling
    
    This fixes the "Length mismatch: Expected axis has 8 elements, new values have 3 elements" error in the forecast processing pipeline by properly detecting and mapping the forecast output columns to quantiles.

commit 94db009b44035a5824cf4b0fc4233ea9a2cff803
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Thu Nov 21 08:18:32 2024 +0200

    Fix Length Mismatch Error in Forecast Result Processing
    
    - Changed `quantiles` default in Config dataclass from a tuple to a list to ensure type consistency with ConfigModel.
    - Removed incorrect assignment of quantile names to the entire forecast DataFrame (`forecast_df.columns = self.config.quantiles`) to prevent length mismatch.
    - Updated `_get_forecast_result` method to extract only the quantile columns from the forecast output and assign quantile names exclusively to these columns.
    - Ensured that the final forecast DataFrame includes the specified quantiles (`p10`, `p50`, `p90`) without altering other DataFrame columns.
    - Verified that `final_forecast.csv` contains the correct columns: `['ProductId', 'BranchId', 'Currency', 'EffectiveDate', 'ForecastDate', 'p10', 'p50', 'p90']`.
    
    This commit addresses the issue where the script attempted to assign 3 quantile names to a DataFrame with 8 columns, causing a length mismatch error. By ensuring consistent data types and correctly assigning quantile names only to the relevant columns, the pipeline now generates accurate probabilistic forecasts as intended."

commit b6576b7e8ff4312a12b3c3bdf1f3d99895372a7b
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Thu Nov 21 07:37:06 2024 +0200

    feat(forecast): Add point forecast capability aligned with main pipeline
    
    Add new PointForecastGenerator class that enables single-point forecasting
    while maintaining consistency with main pipeline architecture.
    
    Key changes:
    - Match inference data structure with ForecastDate and horizon handling
    - Align transform job configuration (MultiRecord, ContentType)
    - Implement identical results processing and quantile handling
    - Add consistent error handling and monitoring
    - Maintain output format compatibility with main pipeline
    
    Breaking changes: None
    Dependencies: Requires same config.yaml as main pipeline
    
    Related: #<main-pipeline-ticket-number>

commit d2fca6e68f04cb914ad0423267e7bc641284b8aa
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Thu Nov 21 07:17:17 2024 +0200

    Use only specified columns and adjust script accordingly
    
    - Modified 'prepare_data' to use only 'BranchId', 'ProductId', 'Currency', 'EffectiveDate', and 'Demand' columns, ignoring any others in the training data.
    - Ensured 'BranchId' and 'ProductId' are correctly handled even if they are numeric by converting them to strings.
    - Updated 'TimeSeriesConfig' in 'train_model' to match the specified columns, removing 'CountryCode' from grouping attributes.
    - Adjusted 'forecast' method and related data processing to use only the specified columns.
    - Simplified 'main()' function to include only the operations necessary for training and forecasting, removing any other operations.
    - Followed best practices by being precise and avoiding unnecessary code enhancements.

commit af12a7a345b4d6a4809f8883938b555cbbbafd4f
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Thu Nov 21 07:03:38 2024 +0200

    Implement transform job monitoring, add backtesting parameter to run_pipeline, and fix directory cleanup
    
    - Added monitoring of batch transform jobs by calling `_monitor_transform_job` after each transform job starts.
    - Updated `run_pipeline` method to accept a `backtesting` parameter and pass it to the `forecast` method.
    - Replaced `os.rmdir` with `shutil.rmtree` for proper temporary directory cleanup.
    - Imported `shutil` module to support directory removal.
    - Ensured that the script correctly handles both backtesting and forecasting from the latest date.
    - Improved exception handling and logging for better debugging and monitoring.

commit 72cc48ddc5459d4c10cd1dd7666b0e0f9320c43c
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Thu Nov 21 06:44:06 2024 +0200

    feat(forecasting): Enhance forecasting pipeline with validation and batch processing
    
    - Add Pydantic model for configuration validation
    - Increase forecast horizon from 1 to 10 days
    - Implement batch processing with configurable batch size
    - Add robust error handling for AWS operations
    - Fix timezone handling in datetime operations
    - Add data validation checks
    - Enhance logging and cleanup operations
    
    This commit improves the reliability and scalability of the forecasting
    pipeline while adding proper validation at multiple levels.

commit 93183828de77dfc12f03800db4736dd6b71835c8
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Thu Nov 21 06:36:38 2024 +0200

    fix: improve cash forecast model robustness and error handling
    
    - Handle zero/negative demand values using min positive value scaling
    - Remove duplicate log transformation of demand data
    - Fix pandas warnings (SettingWithCopyWarning, deprecated applymap)
    - Ensure non-negative predictions and proper quantile ordering (p10小50小90)
    - Add logging for problematic demand values
    
    Technical fixes:
    - Replace zeros/negatives with 0.01 * min_positive_demand
    - Move log1p transform to prepare_data() only
    - Use proper DataFrame creation instead of slice modification
    - Add validation for prediction quantiles after inverse transform

commit 0b144fca4024025a924716ae945f8d9ef80a4f32
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Tue Nov 19 07:24:29 2024 +0200

    fix: improve cash forecast model robustness and error handling
    
    - Handle zero/negative demand values using min positive value scaling
    - Remove duplicate log transformation of demand data
    - Fix pandas warnings (SettingWithCopyWarning, deprecated applymap)
    - Ensure non-negative predictions and proper quantile ordering (p10小50小90)
    - Add logging for problematic demand values
    
    Technical fixes:
    - Replace zeros/negatives with 0.01 * min_positive_demand
    - Move log1p transform to prepare_data() only
    - Use proper DataFrame creation instead of slice modification
    - Add validation for prediction quantiles after inverse transform

commit 2181381c4bc541ee7ad6f65f9e68328302c6692e
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Tue Nov 19 06:51:58 2024 +0200

    Mode changes.

commit 6529edcecf931c278cbf0083fbcf19dea559f007
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Mon Nov 18 17:36:25 2024 +0200

    The days changes

commit 8481a43f6758a8e31a8aa829969373ae1215d1c3
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Mon Nov 18 08:22:50 2024 +0200

    feat(forecasting): Add model reuse capability and standalone script
    
    This commit introduces significant improvements to the cash forecasting pipeline:
    
    1. New Features:
    - Add ability to reuse existing trained models
    - Create standalone script for inference-only pipeline
    - Implement efficient batch processing for forecasts
    - Add robust error handling and logging
    
    2. Performance Improvements:
    - Skip redundant model training phase
    - Optimize memory usage in batch processing
    - Improve S3 interaction efficiency
    
    3. Technical Enhancements:
    - Add Pydantic validation for configuration
    - Implement proper cleanup of temporary files
    - Add detailed logging for debugging
    - Improve error messages and exception handling
    
    4. Usage Changes:
    - Add new --model-name parameter for specifying existing model
    - Maintain backward compatibility with original script
    - Improve documentation and logging output
    
    5. Bug Fixes:
    - Fix "EffectiveDate not found in axis" error
    - Handle missing columns gracefully
    - Proper cleanup of temporary files
    
    To use the new functionality:
    python reuse_cash_forecast_model.py --config config.yaml --countries ZM --model-name ZM-model-20241118-025905
    
    Testing:
    - Tested with existing model ZM-model-20241118-025905
    - Verified output format matches original implementation
    - Validated forecast generation on sample data
    
    Impact:
    - Reduces processing time by skipping training phase
    - Maintains consistency with original forecast format
    - Improves reliability and error handling

commit ccda9c7dc67d75dbfc825fa13de424555b7c0e34
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Fri Nov 15 07:46:43 2024 +0200

    fix(forecast): Fix quantile handling and improve error management
    
    BREAKING CHANGE: Updates quantile format from decimal to pNN format
    
    Key Changes:
    - Add Pydantic validation for configuration management
    - Fix EffectiveDate column handling in forecast pipeline
    - Update quantile format from decimal (0.1) to pNN format (p10)
    - Add proper error handling and logging throughout pipeline
    - Implement better cleanup of temporary files
    
    Technical Details:
    - Add ConfigModel with Pydantic validation
    - Update Config dataclass to match ConfigModel structure
    - Fix forecast result processing in _get_forecast_result
    - Improve date handling and conversion
    - Add debug logging for data transformations
    - Implement proper cleanup in finally blocks
    - Add proper exception handling with specific error messages
    
    Testing:
    - Tested with ZM country code
    - Verified successful model training
    - Confirmed proper forecast generation
    - Validated output file structure
    
    Configuration Updates:
    - quantiles now use 'pNN' format (e.g., 'p10', 'p50', 'p90')
    - Added iterative_forecast_steps parameter
    - Updated validation for all config parameters
    
    Fixes:
    - #1: EffectiveDate column missing error
    - #2: Improper cleanup of temporary files
    - #3: Configuration validation errors
    - #4: Quantile format mismatch in forecasts
    
    Co-authored-by: Assistant <anthropic-claude>

commit 5fd9748babf164d91206cba6f15ec3ca88f9a972
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Fri Nov 15 04:41:02 2024 +0200

    fix: Resolve 15 critical issues for robust cash forecasting pipeline
    
    - **Initialize `self.output_dir`** in `prepare_data` to prevent uninitialized directory errors.
    - **Ensure directory creation** for `self.output_dir` before saving files to avoid `FileNotFoundError`.
    - **Generate unique `transform_job_name`** by appending `batch_number` to prevent SageMaker job name conflicts.
    - **Import `glob` module** to handle multiple inference files correctly.
    - **Clean up temporary files** after processing to prevent memory leaks and optimize performance.
    - **Make forecast quantiles configurable** via `Config` class to handle dynamic quantile requirements.
    - **Add exception handling** around all AWS service calls to manage network and service-related errors gracefully.
    - **Make `batch_size` configurable** through the `Config` class for flexibility based on dataset size and system capacity.
    - **Prevent resource leaks** by removing temporary files and ensuring proper file handling.
    - **Handle timezones** in datetime operations by localizing to `None` to ensure consistent date calculations.
    - **Implement exponential backoff** in monitoring methods to efficiently detect job completions and handle delays.
    - **Wait for model availability** post-training by looping until the best candidate model is accessible.
    - **Prevent output file overwriting** by appending an additional timestamp to `output_file` names.
    - **Validate input data schema** by checking for required columns before processing to avoid runtime errors.
    - **Enhance logging configuration** with specific log levels and formats for better traceability and debugging.
    
    Additional Improvements:
    - **Refactored `forecast` method** to include batching, improving scalability and performance.
    - **Consistent file path usage** across all methods by utilizing `self.output_dir`.
    - **Removed duplicate code** in `_save_forecasts` to streamline the method and reduce maintenance overhead.
    - **Enhanced code readability and maintainability** through structured logging and clear method separations.

commit 434ab02a72d638011680bb81ea0cddb1f4021326
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Wed Nov 13 05:06:37 2024 +0200

    feat: Implement multi-step forecasting for each EffectiveDate with confidence intervals
    
    - Updated the pipeline to generate 10-day forecasts starting from each EffectiveDate in the training data
    - Modified the 'forecast' method to prepare inference data for each EffectiveDate and run batch transforms
    - Adjusted the '_get_forecast_result' method to combine inference data with forecasts correctly
    - Refactored '_save_forecasts' to pivot forecast data and include confidence intervals using quantiles
    - Ensured actual demand and forecasted demand (p10, p50, p90) are included for each ProductId, Currency, BranchId, and EffectiveDate combination
    - Improved logging and error handling throughout the pipeline
    - Removed obsolete methods and parameters related to iterative forecasting
    - Enhanced code comments and documentation for better clarity

commit da6bf94e5aee3b9de2c7d22c739ada90db3d6988
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Sun Nov 10 19:32:07 2024 +0200

    fix(automation): Complete main function and add robust error handling
    
    Complete the script execution flow with comprehensive main function:
    - Add data validation and error checking
    - Implement detailed performance metrics calculation
    - Add structured console output formatting
    - Create comprehensive metrics reporting file
    
    Technical additions:
    - Data path validation and file existence checks
    - Required column validation
    - Detailed statistical calculations (min, max, median)
    - Structured performance metrics output file
    
    Output enhancements:
    - Timestamped directories for better organization
    - Detailed console summary statistics
    - Performance metrics saved to text file
    - Clear success/failure reporting
    
    Error handling:
    - File not found handling
    - Missing column validation
    - Try-catch wrapper for main execution
    - Informative error messages
    
    Path: forecast_analysis.py

commit 2cfe5aa7cc2e2f7fd0ddacd61d73f7ffef2ff969
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Sun Nov 10 19:15:20 2024 +0200

    feat(automation): Add automated multi-combination forecast visualization generator
    
    Implement AutomatedForecastComparison class to batch process forecast visualizations
    - Add parallel processing for multiple product/branch/currency combinations
    - Implement organized directory structure for output management
    - Add comprehensive logging and error handling
    - Generate summary statistics and performance metrics
    - Include progress tracking and completion reporting
    
    Technical details:
    - Uses concurrent.futures for parallel processing
    - Implements Path for robust file handling
    - Adds type hints for better code maintainability
    - Includes detailed logging configuration
    - Generates both visualizations and summary statistics
    
    Key functionality:
    - Automatic processing of all unique combinations
    - MAPE and coverage calculations for each combination
    - Interactive HTML output with Plotly
    - Summary CSV generation with performance metrics
    - Progress tracking and error reporting
    
    Dependencies:
    - plotly
    - pandas
    - pathlib
    - concurrent.futures
    
    Note: Consider increasing max_workers based on available CPU cores

commit 08e5113daaf2f9c192954e8a62c93bf4042ac514
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Sun Nov 10 19:05:50 2024 +0200

    feat(visualization): Add forecast vs actuals comparison tool
    
    Create ForecastComparison class for interactive demand visualization
    - Add detailed time series plots with confidence intervals (p10-p90)
    - Implement product/branch/currency specific filtering
    - Add interactive date range selector and zoom capabilities
    - Include MAPE and coverage metrics in visualization
    - Support hover tooltips with detailed values
    
    Technical details:
    - Uses Plotly for interactive visualizations
    - Implements date range filtering
    - Adds comprehensive type hints
    - Includes example usage in main()
    
    Dependencies:
    - plotly
    - pandas

commit 8849970d5d1610dfae6411878b2e40eba60c94fa
Author: ChristoGH <christo.w.strydom@gmail.com>
Date:   Tue Sep 10 10:59:58 2024 +0200

    korridor cashflow forecasting graphs
    
    Create diesel_demand_all_countries.ipynb to graph
     cashflow with forecasting for all countries
